{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Regression Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pandas, numpy, matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files as dfs\n",
    "mpls_df = pd.read_csv('resources/mpls_solar_weather.csv')\n",
    "olg_df = pd.read_csv('resources/olg_solar_weather.csv')\n",
    "\n",
    "# randomize rows in df\n",
    "mpls_df = mpls_df.sample(frac=1).reset_index(drop=True)\n",
    "olg_df = olg_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "features = [\n",
    "    'clouds_all', 'temp_f', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'hour', 'day_of_year',\n",
    "    'month', 'sin_day', 'cos_day', 'sin_hour', 'cos_hour', 'sin_month', 'cos_month', 'dl_sec'\n",
    "]\n",
    "\n",
    "# parameter to predict\n",
    "target = 'power_delivered'\n",
    "\n",
    "# get input dimensions\n",
    "input_dim = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model():\n",
    "    \n",
    "    '''Build model with 2 hidden layers'''\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=input_dim * 4, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(units=input_dim * 2, activation='relu'))\n",
    "    model.add(Dense(units=input_dim, activation='relu'))\n",
    "    model.add(Dense(units=1))  # 1 output\n",
    "    \n",
    "    # compile with mean-squared error loss function\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_target(df):\n",
    "    '''Takes in df and returns features and target dataframes for training and validation.'''\n",
    "    X = df[features].copy()\n",
    "    y = df[target].copy()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(X, y, k):\n",
    "    '''Performs k-fold cross-validation on data. Outputs array of r2 score results.'''\n",
    "    \n",
    "    kfold = KFold(n_splits=k, random_state=42)\n",
    "    results = cross_val_score(nn_reg, X, y, cv=kfold, scoring='r2', verbose=2, n_jobs=-1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters for regressor\n",
    "model = deep_model\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "# build pipeline using standard scaler\n",
    "nn_reg = Pipeline(steps=[\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('estimator', KerasRegressor(build_fn=model,\n",
    "                                   epochs=epochs,\n",
    "                                   batch_size=batch_size))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 10.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "# dict of solar datasets to loop over\n",
    "dfs_dict = {'mpls': mpls_df,\n",
    "       'olg': olg_df}\n",
    "\n",
    "# dict to store results\n",
    "cv_results = {}\n",
    "\n",
    "# perform cross validation on datasets\n",
    "for key in dfs_dict:\n",
    "    # get features and target\n",
    "    X, y = get_features_target(dfs_dict[key])\n",
    "    \n",
    "    # perform 10-fold cross-validation, store in dict\n",
    "    cv_results[key] = k_fold_cv(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for mpls solar setup:\n",
      "  Model: Deep Learning Neural Net (3 hidden layers), Epochs: 300, Batch Size: 64\n",
      "\n",
      "10-fold cross-validation:\n",
      "  r2 score: 0.89 (0.01)\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "Results for olg solar setup:\n",
      "  Model: Deep Learning Neural Net (3 hidden layers), Epochs: 300, Batch Size: 64\n",
      "\n",
      "10-fold cross-validation:\n",
      "  r2 score: 0.89 (0.02)\n",
      "-------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results for each solar setup\n",
    "for key in dfs_dict:\n",
    "    print(f\"Results for {key} solar setup:\")\n",
    "    print(f\"  Model: Deep Learning Neural Net (3 hidden layers), Epochs: {epochs}, Batch Size: {batch_size}\")\n",
    "    print()\n",
    "    print(\"10-fold cross-validation:\\n  r2 score: %.2f (%.2f)\" % \\\n",
    "          (cv_results[key].mean(), cv_results[key].std()))\n",
    "    print(\"-------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 1155058.0467 - mse: 1155058.0000\n",
      "Epoch 2/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 372584.4342 - mse: 372584.4375\n",
      "Epoch 3/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 275528.4380 - mse: 275528.5625\n",
      "Epoch 4/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 245916.5429 - mse: 245916.5938\n",
      "Epoch 5/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 231607.0554 - mse: 231607.1250\n",
      "Epoch 6/300\n",
      "19859/19859 [==============================] - 1s 25us/step - loss: 221790.2727 - mse: 221790.2812\n",
      "Epoch 7/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 214875.3663 - mse: 214875.4531\n",
      "Epoch 8/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 208783.7858 - mse: 208783.8438\n",
      "Epoch 9/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 203976.7969 - mse: 203976.7656\n",
      "Epoch 10/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 200054.7498 - mse: 200054.7812\n",
      "Epoch 11/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 195794.0513 - mse: 195794.0000\n",
      "Epoch 12/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 192008.5030 - mse: 192008.5000\n",
      "Epoch 13/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 189040.5551 - mse: 189040.5312\n",
      "Epoch 14/300\n",
      "19859/19859 [==============================] - 1s 25us/step - loss: 186286.0141 - mse: 186286.0781\n",
      "Epoch 15/300\n",
      "19859/19859 [==============================] - 1s 25us/step - loss: 184197.4594 - mse: 184197.5312\n",
      "Epoch 16/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 182777.8358 - mse: 182777.7812\n",
      "Epoch 17/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 180346.1429 - mse: 180346.0781\n",
      "Epoch 18/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 177292.2723 - mse: 177292.2969\n",
      "Epoch 19/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 177175.3424 - mse: 177175.2500\n",
      "Epoch 20/300\n",
      "19859/19859 [==============================] - 1s 37us/step - loss: 175533.9187 - mse: 175533.9219\n",
      "Epoch 21/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 173900.4049 - mse: 173900.3906\n",
      "Epoch 22/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 172406.8957 - mse: 172406.9375\n",
      "Epoch 23/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 171166.0914 - mse: 171166.0625\n",
      "Epoch 24/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 170825.1640 - mse: 170825.1094\n",
      "Epoch 25/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 169390.1208 - mse: 169390.1562\n",
      "Epoch 26/300\n",
      "19859/19859 [==============================] - 1s 35us/step - loss: 169030.6095 - mse: 169030.6406\n",
      "Epoch 27/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 167043.5902 - mse: 167043.5312\n",
      "Epoch 28/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 165669.2839 - mse: 165669.2812\n",
      "Epoch 29/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 164619.4254 - mse: 164619.4219\n",
      "Epoch 30/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 163967.9365 - mse: 163967.9531\n",
      "Epoch 31/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 162924.5776 - mse: 162924.6250\n",
      "Epoch 32/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 161565.5708 - mse: 161565.5625\n",
      "Epoch 33/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 161881.6758 - mse: 161881.6875\n",
      "Epoch 34/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 160880.0103 - mse: 160879.9688\n",
      "Epoch 35/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 159994.7824 - mse: 159994.7812\n",
      "Epoch 36/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 159338.9495 - mse: 159338.9688\n",
      "Epoch 37/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 158179.0164 - mse: 158179.0000\n",
      "Epoch 38/300\n",
      "19859/19859 [==============================] - 1s 32us/step - loss: 157408.2689 - mse: 157408.3594\n",
      "Epoch 39/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 156722.1047 - mse: 156722.1562\n",
      "Epoch 40/300\n",
      "19859/19859 [==============================] - 1s 31us/step - loss: 155467.0817 - mse: 155467.0781\n",
      "Epoch 41/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 154826.8128 - mse: 154826.7812\n",
      "Epoch 42/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 155069.4732 - mse: 155069.5156\n",
      "Epoch 43/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 153696.2239 - mse: 153696.2188\n",
      "Epoch 44/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 153516.9325 - mse: 153516.8750\n",
      "Epoch 45/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 152875.8876 - mse: 152875.9219\n",
      "Epoch 46/300\n",
      "19859/19859 [==============================] - 1s 31us/step - loss: 152955.8358 - mse: 152955.7656\n",
      "Epoch 47/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 151492.1898 - mse: 151492.1094\n",
      "Epoch 48/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 152118.2156 - mse: 152118.2031\n",
      "Epoch 49/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 150771.6021 - mse: 150771.5938\n",
      "Epoch 50/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 150693.6339 - mse: 150693.5469\n",
      "Epoch 51/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 149813.2257 - mse: 149813.2031\n",
      "Epoch 52/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 148815.6648 - mse: 148815.5938\n",
      "Epoch 53/300\n",
      "19859/19859 [==============================] - 1s 45us/step - loss: 149097.6738 - mse: 149097.6719\n",
      "Epoch 54/300\n",
      "19859/19859 [==============================] - 1s 37us/step - loss: 149446.9165 - mse: 149446.8906\n",
      "Epoch 55/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 148470.5390 - mse: 148470.5312\n",
      "Epoch 56/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 147785.2119 - mse: 147785.1875\n",
      "Epoch 57/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 147374.4574 - mse: 147374.5312\n",
      "Epoch 58/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 146905.3515 - mse: 146905.3125\n",
      "Epoch 59/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 147101.1221 - mse: 147101.0781\n",
      "Epoch 60/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 146663.1566 - mse: 146663.1562\n",
      "Epoch 61/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 146874.1013 - mse: 146874.0781\n",
      "Epoch 62/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 145812.2750 - mse: 145812.1875\n",
      "Epoch 63/300\n",
      "19859/19859 [==============================] - 1s 32us/step - loss: 145421.4667 - mse: 145421.5312\n",
      "Epoch 64/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 145361.0350 - mse: 145361.0625\n",
      "Epoch 65/300\n",
      "19859/19859 [==============================] - 1s 30us/step - loss: 145666.0437 - mse: 145666.0938\n",
      "Epoch 66/300\n",
      "19859/19859 [==============================] - 1s 28us/step - loss: 144574.0830 - mse: 144574.2188\n",
      "Epoch 67/300\n",
      "19859/19859 [==============================] - 1s 32us/step - loss: 144374.2021 - mse: 144374.2812\n",
      "Epoch 68/300\n",
      "19859/19859 [==============================] - 1s 33us/step - loss: 143320.7007 - mse: 143320.6875\n",
      "Epoch 69/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 144988.2161 - mse: 144988.1406\n",
      "Epoch 70/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 142470.4450 - mse: 142470.4531\n",
      "Epoch 71/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 142880.8770 - mse: 142880.9688\n",
      "Epoch 72/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 142917.3599 - mse: 142917.3125\n",
      "Epoch 73/300\n",
      "19859/19859 [==============================] - 1s 29us/step - loss: 143000.8821 - mse: 143000.9688\n",
      "Epoch 74/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 143855.7830 - mse: 143855.7500\n",
      "Epoch 75/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 142023.6995 - mse: 142023.7500\n",
      "Epoch 76/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 142849.0301 - mse: 142849.0469\n",
      "Epoch 77/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 141328.3305 - mse: 141328.3125\n",
      "Epoch 78/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 141059.2402 - mse: 141059.2656\n",
      "Epoch 79/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 141092.4332 - mse: 141092.4844\n",
      "Epoch 80/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 141008.8588 - mse: 141008.8125\n",
      "Epoch 81/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 140474.1283 - mse: 140474.1562\n",
      "Epoch 82/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 140489.9125 - mse: 140489.9375\n",
      "Epoch 83/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 139923.1950 - mse: 139923.2031\n",
      "Epoch 84/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 140037.2672 - mse: 140037.2500\n",
      "Epoch 85/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 140044.7117 - mse: 140044.6719\n",
      "Epoch 86/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 139656.4413 - mse: 139656.3906\n",
      "Epoch 87/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 139515.9432 - mse: 139515.8906\n",
      "Epoch 88/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 138976.3336 - mse: 138976.3125\n",
      "Epoch 89/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 138607.2340 - mse: 138607.2344\n",
      "Epoch 90/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 137953.0063 - mse: 137953.0312\n",
      "Epoch 91/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 138856.5286 - mse: 138856.5156\n",
      "Epoch 92/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 138354.9437 - mse: 138354.9062\n",
      "Epoch 93/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 138045.1795 - mse: 138045.1562\n",
      "Epoch 94/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 137716.9876 - mse: 137716.9531\n",
      "Epoch 95/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 137114.3302 - mse: 137114.3906\n",
      "Epoch 96/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 136443.8181 - mse: 136443.7656\n",
      "Epoch 97/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 137270.5754 - mse: 137270.5312\n",
      "Epoch 98/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 135979.5288 - mse: 135979.4375\n",
      "Epoch 99/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 135911.2097 - mse: 135911.1562\n",
      "Epoch 100/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 135719.2726 - mse: 135719.3125\n",
      "Epoch 101/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 136386.3462 - mse: 136386.2656\n",
      "Epoch 102/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 135995.1419 - mse: 135995.1250\n",
      "Epoch 103/300\n",
      "19859/19859 [==============================] - 1s 25us/step - loss: 136020.6199 - mse: 136020.5625\n",
      "Epoch 104/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 136156.5503 - mse: 136156.5938\n",
      "Epoch 105/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 135162.9203 - mse: 135162.9531\n",
      "Epoch 106/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 135250.4286 - mse: 135250.5000\n",
      "Epoch 107/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 134537.8951 - mse: 134537.8750\n",
      "Epoch 108/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 135790.5474 - mse: 135790.5781\n",
      "Epoch 109/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 134609.3424 - mse: 134609.3125\n",
      "Epoch 110/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 134116.2734 - mse: 134116.2656\n",
      "Epoch 111/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 134462.5396 - mse: 134462.5469\n",
      "Epoch 112/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 133308.2958 - mse: 133308.2656\n",
      "Epoch 113/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 133383.0827 - mse: 133383.0938\n",
      "Epoch 114/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 134726.7224 - mse: 134726.7344\n",
      "Epoch 115/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 133070.8968 - mse: 133070.8281\n",
      "Epoch 116/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132596.2886 - mse: 132596.2656\n",
      "Epoch 117/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132534.1397 - mse: 132534.2031\n",
      "Epoch 118/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132520.8390 - mse: 132520.8750\n",
      "Epoch 119/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132234.1682 - mse: 132234.1562\n",
      "Epoch 120/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132847.9198 - mse: 132847.9062\n",
      "Epoch 121/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131916.7675 - mse: 131916.7969\n",
      "Epoch 122/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 133428.5842 - mse: 133428.5625\n",
      "Epoch 123/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131949.3460 - mse: 131949.3125\n",
      "Epoch 124/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 132777.0727 - mse: 132777.0781\n",
      "Epoch 125/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 132674.5316 - mse: 132674.5156\n",
      "Epoch 126/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 132313.6563 - mse: 132313.6406\n",
      "Epoch 127/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 132369.2356 - mse: 132369.2812\n",
      "Epoch 128/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131280.7059 - mse: 131280.6875\n",
      "Epoch 129/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130539.1847 - mse: 130539.2031\n",
      "Epoch 130/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131463.7685 - mse: 131463.7656\n",
      "Epoch 131/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130593.0643 - mse: 130593.0547\n",
      "Epoch 132/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131285.0544 - mse: 131285.1250\n",
      "Epoch 133/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131570.3617 - mse: 131570.3750\n",
      "Epoch 134/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130289.6532 - mse: 130289.6406\n",
      "Epoch 135/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130336.9290 - mse: 130336.9375\n",
      "Epoch 136/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 131097.4725 - mse: 131097.4531\n",
      "Epoch 137/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 129781.3372 - mse: 129781.2969\n",
      "Epoch 138/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130474.0543 - mse: 130474.0859\n",
      "Epoch 139/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 129836.1318 - mse: 129836.1094\n",
      "Epoch 140/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 129708.8921 - mse: 129708.8828\n",
      "Epoch 141/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 130314.2315 - mse: 130314.2734\n",
      "Epoch 142/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 129459.4695 - mse: 129459.4766\n",
      "Epoch 143/300\n",
      "19859/19859 [==============================] - 1s 27us/step - loss: 129712.4865 - mse: 129712.3828\n",
      "Epoch 144/300\n",
      "19859/19859 [==============================] - 1s 26us/step - loss: 129175.0872 - mse: 129175.1094\n",
      "Epoch 145/300\n",
      "19859/19859 [==============================] - 0s 25us/step - loss: 129519.0285 - mse: 129519.0391\n",
      "Epoch 146/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19859/19859 [==============================] - 0s 22us/step - loss: 129778.9554 - mse: 129778.9062\n",
      "Epoch 147/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 129784.2783 - mse: 129784.2812\n",
      "Epoch 148/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128949.4296 - mse: 128949.4297\n",
      "Epoch 149/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128522.0954 - mse: 128522.1562\n",
      "Epoch 150/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 129417.6501 - mse: 129417.6875\n",
      "Epoch 151/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128595.2589 - mse: 128595.2109\n",
      "Epoch 152/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128324.6120 - mse: 128324.5703\n",
      "Epoch 153/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128522.4147 - mse: 128522.4531\n",
      "Epoch 154/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128077.6193 - mse: 128077.5938\n",
      "Epoch 155/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 127693.1578 - mse: 127693.1172\n",
      "Epoch 156/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 127971.7219 - mse: 127971.7422\n",
      "Epoch 157/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128285.1895 - mse: 128285.1875\n",
      "Epoch 158/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 127687.8752 - mse: 127687.8359\n",
      "Epoch 159/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 128219.7942 - mse: 128219.7734\n",
      "Epoch 160/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 127458.8779 - mse: 127458.8281\n",
      "Epoch 161/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 127569.6497 - mse: 127569.6641\n",
      "Epoch 162/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 127356.1736 - mse: 127356.2188\n",
      "Epoch 163/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 126285.1843 - mse: 126285.1797\n",
      "Epoch 164/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126230.3472 - mse: 126230.3750\n",
      "Epoch 165/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126577.8627 - mse: 126577.8750\n",
      "Epoch 166/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126426.8128 - mse: 126426.8125\n",
      "Epoch 167/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 125930.0265 - mse: 125930.0859\n",
      "Epoch 168/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 125258.1201 - mse: 125258.0859\n",
      "Epoch 169/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 125687.2394 - mse: 125687.2500\n",
      "Epoch 170/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 125501.5834 - mse: 125501.5547\n",
      "Epoch 171/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126216.3398 - mse: 126216.3047\n",
      "Epoch 172/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126712.6835 - mse: 126712.6875\n",
      "Epoch 173/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126296.7496 - mse: 126296.7266\n",
      "Epoch 174/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 125968.3092 - mse: 125968.3203\n",
      "Epoch 175/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 125641.7812 - mse: 125641.7969\n",
      "Epoch 176/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124548.3181 - mse: 124548.2891\n",
      "Epoch 177/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124807.7794 - mse: 124807.7656\n",
      "Epoch 178/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 125506.3663 - mse: 125506.3203\n",
      "Epoch 179/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123903.4285 - mse: 123903.5000\n",
      "Epoch 180/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 126267.9759 - mse: 126268.0234\n",
      "Epoch 181/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124140.8860 - mse: 124140.9062\n",
      "Epoch 182/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124711.3785 - mse: 124711.3828\n",
      "Epoch 183/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124063.9601 - mse: 124063.9609\n",
      "Epoch 184/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124524.7112 - mse: 124524.7188\n",
      "Epoch 185/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 125439.8497 - mse: 125439.8594\n",
      "Epoch 186/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123498.3503 - mse: 123498.3594\n",
      "Epoch 187/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124738.3278 - mse: 124738.3594\n",
      "Epoch 188/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123310.6758 - mse: 123310.6719\n",
      "Epoch 189/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 124917.3830 - mse: 124917.4297\n",
      "Epoch 190/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 123915.2227 - mse: 123915.2266\n",
      "Epoch 191/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 124052.6471 - mse: 124052.7109\n",
      "Epoch 192/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 123736.7538 - mse: 123736.7891\n",
      "Epoch 193/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123223.5040 - mse: 123223.5000\n",
      "Epoch 194/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 122885.8027 - mse: 122885.8125\n",
      "Epoch 195/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122848.7994 - mse: 122848.8281\n",
      "Epoch 196/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122636.6988 - mse: 122636.7109\n",
      "Epoch 197/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122403.5599 - mse: 122403.5547\n",
      "Epoch 198/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122772.0462 - mse: 122772.0781\n",
      "Epoch 199/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122818.1646 - mse: 122818.1484\n",
      "Epoch 200/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122825.5715 - mse: 122825.5078\n",
      "Epoch 201/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123899.0508 - mse: 123899.0469\n",
      "Epoch 202/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123169.3902 - mse: 123169.3359\n",
      "Epoch 203/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 121644.1054 - mse: 121644.0469\n",
      "Epoch 204/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122331.7520 - mse: 122331.7656\n",
      "Epoch 205/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122704.0693 - mse: 122704.0781\n",
      "Epoch 206/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 122209.3561 - mse: 122209.3125\n",
      "Epoch 207/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122747.3354 - mse: 122747.3047\n",
      "Epoch 208/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 122024.8865 - mse: 122024.8594\n",
      "Epoch 209/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 121584.3647 - mse: 121584.3672\n",
      "Epoch 210/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 123047.0434 - mse: 123047.0391\n",
      "Epoch 211/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 121577.3726 - mse: 121577.3750\n",
      "Epoch 212/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 121623.0638 - mse: 121623.0391\n",
      "Epoch 213/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 121159.3931 - mse: 121159.4062\n",
      "Epoch 214/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120998.8186 - mse: 120998.8750\n",
      "Epoch 215/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 121054.9273 - mse: 121054.9453\n",
      "Epoch 216/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 121251.0962 - mse: 121251.0312\n",
      "Epoch 217/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 120481.5347 - mse: 120481.5781\n",
      "Epoch 218/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 121888.4185 - mse: 121888.3281\n",
      "Epoch 219/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120824.5763 - mse: 120824.6016\n",
      "Epoch 220/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120673.1300 - mse: 120673.1328\n",
      "Epoch 221/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120444.4866 - mse: 120444.4766\n",
      "Epoch 222/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120362.4335 - mse: 120362.3984\n",
      "Epoch 223/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120575.9226 - mse: 120575.9062: 0s - loss: 123723.2252 - mse: 123723\n",
      "Epoch 224/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 121290.6010 - mse: 121290.5781\n",
      "Epoch 225/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120688.5217 - mse: 120688.5000\n",
      "Epoch 226/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 121162.5872 - mse: 121162.6094\n",
      "Epoch 227/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120447.7092 - mse: 120447.6953\n",
      "Epoch 228/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120670.2956 - mse: 120670.3125\n",
      "Epoch 229/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 122285.1502 - mse: 122285.1484\n",
      "Epoch 230/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 122013.7389 - mse: 122013.7969\n",
      "Epoch 231/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120473.0382 - mse: 120473.0312\n",
      "Epoch 232/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120924.6781 - mse: 120924.7109\n",
      "Epoch 233/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120364.3698 - mse: 120364.3281\n",
      "Epoch 234/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 120088.2650 - mse: 120088.2500\n",
      "Epoch 235/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 119600.4240 - mse: 119600.3828\n",
      "Epoch 236/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120128.8266 - mse: 120128.8281\n",
      "Epoch 237/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 119469.2716 - mse: 119469.2656\n",
      "Epoch 238/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 119164.4605 - mse: 119164.4609\n",
      "Epoch 239/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120142.0836 - mse: 120142.0312\n",
      "Epoch 240/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 119783.3928 - mse: 119783.3359\n",
      "Epoch 241/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120100.2662 - mse: 120100.2734\n",
      "Epoch 242/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 119899.2921 - mse: 119899.2969\n",
      "Epoch 243/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 118998.5672 - mse: 118998.5938\n",
      "Epoch 244/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118530.5566 - mse: 118530.5547\n",
      "Epoch 245/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 120257.9547 - mse: 120257.9297\n",
      "Epoch 246/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118645.0661 - mse: 118645.0469\n",
      "Epoch 247/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118595.0951 - mse: 118595.0938\n",
      "Epoch 248/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118401.4252 - mse: 118401.4375\n",
      "Epoch 249/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118638.6079 - mse: 118638.5781\n",
      "Epoch 250/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 119197.9811 - mse: 119197.9688\n",
      "Epoch 251/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118194.7736 - mse: 118194.7578\n",
      "Epoch 252/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 119549.2781 - mse: 119549.2812\n",
      "Epoch 253/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118567.8616 - mse: 118567.8984\n",
      "Epoch 254/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 119274.1579 - mse: 119274.1641\n",
      "Epoch 255/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118393.2212 - mse: 118393.1719\n",
      "Epoch 256/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 117540.8871 - mse: 117540.8828\n",
      "Epoch 257/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 117370.3913 - mse: 117370.4531\n",
      "Epoch 258/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 119050.4269 - mse: 119050.4531\n",
      "Epoch 259/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 118102.4761 - mse: 118102.5078\n",
      "Epoch 260/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118351.3851 - mse: 118351.4062\n",
      "Epoch 261/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118311.9168 - mse: 118311.9766\n",
      "Epoch 262/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 117766.6354 - mse: 117766.6250\n",
      "Epoch 263/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 117890.2884 - mse: 117890.3359\n",
      "Epoch 264/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118127.1977 - mse: 118127.1328\n",
      "Epoch 265/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 117541.7290 - mse: 117541.7188\n",
      "Epoch 266/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 117165.3557 - mse: 117165.3984\n",
      "Epoch 267/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 117999.0646 - mse: 117999.0625\n",
      "Epoch 268/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 116729.0276 - mse: 116729.0391\n",
      "Epoch 269/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 117758.1058 - mse: 117758.1016\n",
      "Epoch 270/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118021.2040 - mse: 118021.1797\n",
      "Epoch 271/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118060.5856 - mse: 118060.5781\n",
      "Epoch 272/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116908.6887 - mse: 116908.6719\n",
      "Epoch 273/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 117461.6787 - mse: 117461.7031\n",
      "Epoch 274/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 117186.3177 - mse: 117186.3203\n",
      "Epoch 275/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116884.4859 - mse: 116884.5000\n",
      "Epoch 276/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116917.3138 - mse: 116917.3750\n",
      "Epoch 277/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 117534.0072 - mse: 117534.0234\n",
      "Epoch 278/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 117025.0252 - mse: 117025.0547\n",
      "Epoch 279/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116634.8565 - mse: 116634.8594\n",
      "Epoch 280/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116976.4755 - mse: 116976.4531\n",
      "Epoch 281/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 116974.8524 - mse: 116974.8438\n",
      "Epoch 282/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116976.7186 - mse: 116976.7109\n",
      "Epoch 283/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116665.5430 - mse: 116665.5234\n",
      "Epoch 284/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116725.5235 - mse: 116725.5078\n",
      "Epoch 285/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 116570.5268 - mse: 116570.5547\n",
      "Epoch 286/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116433.3024 - mse: 116433.3203\n",
      "Epoch 287/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 117433.6079 - mse: 117433.5625\n",
      "Epoch 288/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116687.4795 - mse: 116687.5156\n",
      "Epoch 289/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 115992.2754 - mse: 115992.2891\n",
      "Epoch 290/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19859/19859 [==============================] - 0s 22us/step - loss: 116226.8152 - mse: 116226.7734\n",
      "Epoch 291/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116042.5451 - mse: 116042.5625\n",
      "Epoch 292/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 116734.0767 - mse: 116734.0938\n",
      "Epoch 293/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116642.1307 - mse: 116642.1250\n",
      "Epoch 294/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 118168.6844 - mse: 118168.6953\n",
      "Epoch 295/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116357.8042 - mse: 116357.8047\n",
      "Epoch 296/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 114973.2867 - mse: 114973.2734\n",
      "Epoch 297/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116246.7971 - mse: 116246.8438\n",
      "Epoch 298/300\n",
      "19859/19859 [==============================] - 0s 22us/step - loss: 116183.3968 - mse: 116183.4297\n",
      "Epoch 299/300\n",
      "19859/19859 [==============================] - 0s 23us/step - loss: 115611.0160 - mse: 115611.0234\n",
      "Epoch 300/300\n",
      "19859/19859 [==============================] - 0s 24us/step - loss: 116553.5711 - mse: 116553.6094\n"
     ]
    }
   ],
   "source": [
    "# mpls data:\n",
    "X, y = get_features_target(mpls_df)   # get data\n",
    "nn_reg.fit(X, y)     # train model\n",
    "nn_reg.named_steps['estimator'].model.save('mpls_nn_reg.h5')  # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2582/2582 [==============================] - 0s 69us/step - loss: 1194011087.6158 - mse: 1194011264.0000\n",
      "Epoch 2/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 1193705952.3718 - mse: 1193705856.0000\n",
      "Epoch 3/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 1191921746.3424 - mse: 1191921920.0000\n",
      "Epoch 4/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 1184260421.6019 - mse: 1184260224.0000\n",
      "Epoch 5/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 1161666675.5074 - mse: 1161666688.0000\n",
      "Epoch 6/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 1111367415.2750 - mse: 1111367552.0000\n",
      "Epoch 7/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 1020469758.2153 - mse: 1020469824.0000\n",
      "Epoch 8/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 884375283.5074 - mse: 884375360.0000\n",
      "Epoch 9/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 715376859.0178 - mse: 715376832.0000\n",
      "Epoch 10/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 558643202.7761 - mse: 558643200.0000\n",
      "Epoch 11/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 441665575.9566 - mse: 441665632.0000\n",
      "Epoch 12/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 373434472.5515 - mse: 373434496.0000\n",
      "Epoch 13/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 332022214.2215 - mse: 332022208.0000\n",
      "Epoch 14/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 301531383.5105 - mse: 301531392.0000\n",
      "Epoch 15/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 277193839.5414 - mse: 277193856.0000\n",
      "Epoch 16/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 256229831.5600 - mse: 256229824.0000\n",
      "Epoch 17/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 238063513.4067 - mse: 238063552.0000\n",
      "Epoch 18/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 222340400.4214 - mse: 222340400.0000\n",
      "Epoch 19/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 208631137.9458 - mse: 208631136.0000\n",
      "Epoch 20/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 196732944.9543 - mse: 196732928.0000\n",
      "Epoch 21/300\n",
      "2582/2582 [==============================] - 0s 38us/step - loss: 186223404.0465 - mse: 186223392.0000\n",
      "Epoch 22/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 177089704.5391 - mse: 177089696.0000\n",
      "Epoch 23/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 169067440.1363 - mse: 169067424.0000\n",
      "Epoch 24/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 162205169.6979 - mse: 162205184.0000\n",
      "Epoch 25/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 156212032.0000 - mse: 156212048.0000\n",
      "Epoch 26/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 150952810.6987 - mse: 150952816.0000\n",
      "Epoch 27/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 146128012.1704 - mse: 146128000.0000\n",
      "Epoch 28/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 141973322.9187 - mse: 141973328.0000\n",
      "Epoch 29/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 138286568.7498 - mse: 138286576.0000\n",
      "Epoch 30/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 135126476.8211 - mse: 135126448.0000\n",
      "Epoch 31/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 132078566.0914 - mse: 132078592.0000\n",
      "Epoch 32/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 129419384.0310 - mse: 129419360.0000\n",
      "Epoch 33/300\n",
      "2582/2582 [==============================] - 0s 38us/step - loss: 127010816.5143 - mse: 127010800.0000\n",
      "Epoch 34/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 124896077.1247 - mse: 124896080.0000\n",
      "Epoch 35/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 122719260.1394 - mse: 122719256.0000\n",
      "Epoch 36/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 120848353.4624 - mse: 120848360.0000\n",
      "Epoch 37/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 119144429.1154 - mse: 119144448.0000\n",
      "Epoch 38/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 117744542.5500 - mse: 117744536.0000\n",
      "Epoch 39/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 116095701.2177 - mse: 116095680.0000\n",
      "Epoch 40/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 114777970.9620 - mse: 114777976.0000\n",
      "Epoch 41/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 113267442.9311 - mse: 113267424.0000\n",
      "Epoch 42/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 112071482.1379 - mse: 112071488.0000\n",
      "Epoch 43/300\n",
      "2582/2582 [==============================] - 0s 38us/step - loss: 110799155.9473 - mse: 110799144.0000\n",
      "Epoch 44/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 109714447.3803 - mse: 109714456.0000\n",
      "Epoch 45/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 108585634.0883 - mse: 108585640.0000\n",
      "Epoch 46/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 107651201.2703 - mse: 107651208.0000\n",
      "Epoch 47/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 106887888.9357 - mse: 106887872.0000\n",
      "Epoch 48/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 105860362.4973 - mse: 105860360.0000\n",
      "Epoch 49/300\n",
      "2582/2582 [==============================] - 0s 50us/step - loss: 104885874.6150 - mse: 104885880.0000\n",
      "Epoch 50/300\n",
      "2582/2582 [==============================] - 0s 68us/step - loss: 104059239.6220 - mse: 104059240.0000\n",
      "Epoch 51/300\n",
      "2582/2582 [==============================] - 0s 79us/step - loss: 103315380.1580 - mse: 103315384.0000\n",
      "Epoch 52/300\n",
      "2582/2582 [==============================] - 0s 61us/step - loss: 102572559.3679 - mse: 102572560.0000\n",
      "Epoch 53/300\n",
      "2582/2582 [==============================] - 0s 49us/step - loss: 101778600.4090 - mse: 101778592.0000\n",
      "Epoch 54/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 101193228.1890 - mse: 101193240.0000\n",
      "Epoch 55/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 100606182.4229 - mse: 100606184.0000\n",
      "Epoch 56/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 99950377.6731 - mse: 99950384.0000\n",
      "Epoch 57/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 99495214.0294 - mse: 99495216.0000\n",
      "Epoch 58/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 98938653.0751 - mse: 98938632.0000\n",
      "Epoch 59/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 98323817.4624 - mse: 98323824.0000\n",
      "Epoch 60/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 97901505.7227 - mse: 97901504.0000\n",
      "Epoch 61/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 97460109.0132 - mse: 97460112.0000\n",
      "Epoch 62/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 96950031.1510 - mse: 96950032.0000\n",
      "Epoch 63/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 96627113.7351 - mse: 96627128.0000\n",
      "Epoch 64/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 96162155.0488 - mse: 96162168.0000\n",
      "Epoch 65/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 95886483.7521 - mse: 95886480.0000\n",
      "Epoch 66/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 95415864.5546 - mse: 95415856.0000\n",
      "Epoch 67/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 95085029.1681 - mse: 95085040.0000\n",
      "Epoch 68/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 94835686.1472 - mse: 94835672.0000\n",
      "Epoch 69/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 94421214.5252 - mse: 94421224.0000\n",
      "Epoch 70/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 94101563.2657 - mse: 94101560.0000\n",
      "Epoch 71/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 93941039.0581 - mse: 93941024.0000\n",
      "Epoch 72/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 93572852.4895 - mse: 93572864.0000\n",
      "Epoch 73/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 93271501.6824 - mse: 93271504.0000\n",
      "Epoch 74/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 93155064.3904 - mse: 93155072.0000\n",
      "Epoch 75/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 92778210.1255 - mse: 92778216.0000\n",
      "Epoch 76/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 92490970.2339 - mse: 92490976.0000\n",
      "Epoch 77/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 92244826.8319 - mse: 92244840.0000\n",
      "Epoch 78/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 91981163.7924 - mse: 91981176.0000\n",
      "Epoch 79/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 91827100.8892 - mse: 91827104.0000\n",
      "Epoch 80/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 91575001.3400 - mse: 91574992.0000\n",
      "Epoch 81/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 91483151.1325 - mse: 91483152.0000\n",
      "Epoch 82/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 91235519.7645 - mse: 91235520.0000\n",
      "Epoch 83/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 91082425.2270 - mse: 91082440.0000\n",
      "Epoch 84/300\n",
      "2582/2582 [==============================] - 0s 38us/step - loss: 90735772.2076 - mse: 90735768.0000\n",
      "Epoch 85/300\n",
      "2582/2582 [==============================] - 0s 40us/step - loss: 90713402.8288 - mse: 90713408.0000\n",
      "Epoch 86/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 90592399.4547 - mse: 90592400.0000\n",
      "Epoch 87/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 90262085.3292 - mse: 90262088.0000\n",
      "Epoch 88/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 90120877.6917 - mse: 90120872.0000\n",
      "Epoch 89/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 89982524.8087 - mse: 89982520.0000\n",
      "Epoch 90/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 89823176.2603 - mse: 89823176.0000\n",
      "Epoch 91/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 89729868.4555 - mse: 89729872.0000\n",
      "Epoch 92/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 89621556.3997 - mse: 89621544.0000\n",
      "Epoch 93/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 89518144.0496 - mse: 89518152.0000\n",
      "Epoch 94/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 89258114.4477 - mse: 89258104.0000\n",
      "Epoch 95/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 89092907.3462 - mse: 89092904.0000\n",
      "Epoch 96/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 89338294.4012 - mse: 89338296.0000\n",
      "Epoch 97/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 88957644.3811 - mse: 88957632.0000\n",
      "Epoch 98/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 88712409.6855 - mse: 88712392.0000\n",
      "Epoch 99/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 88531328.1053 - mse: 88531328.0000\n",
      "Epoch 100/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 88326645.9923 - mse: 88326656.0000\n",
      "Epoch 101/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 88159015.7893 - mse: 88159016.0000\n",
      "Epoch 102/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 88192135.5600 - mse: 88192136.0000\n",
      "Epoch 103/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 88288294.9280 - mse: 88288288.0000\n",
      "Epoch 104/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 87900519.7490 - mse: 87900512.0000\n",
      "Epoch 105/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 87811444.4988 - mse: 87811456.0000\n",
      "Epoch 106/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 87891651.2657 - mse: 87891648.0000\n",
      "Epoch 107/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 87541678.7971 - mse: 87541688.0000\n",
      "Epoch 108/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 87486951.5972 - mse: 87486952.0000\n",
      "Epoch 109/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 87200913.4253 - mse: 87200888.0000\n",
      "Epoch 110/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 87202117.3912 - mse: 87202112.0000\n",
      "Epoch 111/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 86998058.4477 - mse: 86998056.0000\n",
      "Epoch 112/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 86917417.1433 - mse: 86917408.0000\n",
      "Epoch 113/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 87014876.8397 - mse: 87014864.0000\n",
      "Epoch 114/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 86801378.2928 - mse: 86801384.0000\n",
      "Epoch 115/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 86795891.1387 - mse: 86795888.0000\n",
      "Epoch 116/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 86470316.6290 - mse: 86470312.0000\n",
      "Epoch 117/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 86462470.9466 - mse: 86462472.0000\n",
      "Epoch 118/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 86219529.3013 - mse: 86219536.0000\n",
      "Epoch 119/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 86171764.3997 - mse: 86171752.0000\n",
      "Epoch 120/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 85999465.8451 - mse: 85999480.0000\n",
      "Epoch 121/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 86067231.8017 - mse: 86067224.0000\n",
      "Epoch 122/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 85830734.5252 - mse: 85830728.0000\n",
      "Epoch 123/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 85705335.3308 - mse: 85705336.0000\n",
      "Epoch 124/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 85952011.3648 - mse: 85952000.0000\n",
      "Epoch 125/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85747593.6793 - mse: 85747584.0000\n",
      "Epoch 126/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85591371.3989 - mse: 85591360.0000\n",
      "Epoch 127/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85338532.5438 - mse: 85338528.0000\n",
      "Epoch 128/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85459203.4857 - mse: 85459208.0000\n",
      "Epoch 129/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85202756.5856 - mse: 85202736.0000\n",
      "Epoch 130/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 85056545.2084 - mse: 85056536.0000\n",
      "Epoch 131/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 84910591.4361 - mse: 84910600.0000\n",
      "Epoch 132/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 84971210.9063 - mse: 84971216.0000\n",
      "Epoch 133/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 84940621.4005 - mse: 84940616.0000\n",
      "Epoch 134/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 84652837.7568 - mse: 84652832.0000: 0s - loss: 83109766.9091 - mse: 83109760.000\n",
      "Epoch 135/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 84772471.9814 - mse: 84772464.0000\n",
      "Epoch 136/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 84716940.5453 - mse: 84716944.0000\n",
      "Epoch 137/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 84608041.8156 - mse: 84608056.0000\n",
      "Epoch 138/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 84496832.5763 - mse: 84496840.0000\n",
      "Epoch 139/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 84256415.4981 - mse: 84256416.0000\n",
      "Epoch 140/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 84130692.6352 - mse: 84130680.0000\n",
      "Epoch 141/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 84010244.3284 - mse: 84010248.0000\n",
      "Epoch 142/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2582/2582 [==============================] - 0s 27us/step - loss: 84086275.8513 - mse: 84086288.0000\n",
      "Epoch 143/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 83909096.7250 - mse: 83909088.0000\n",
      "Epoch 144/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 83891261.9799 - mse: 83891272.0000\n",
      "Epoch 145/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 83886784.6321 - mse: 83886768.0000\n",
      "Epoch 146/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 83805083.3586 - mse: 83805072.0000\n",
      "Epoch 147/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 83711366.0852 - mse: 83711368.0000\n",
      "Epoch 148/300\n",
      "2582/2582 [==============================] - 0s 53us/step - loss: 83603289.3819 - mse: 83603288.0000\n",
      "Epoch 149/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 83822333.2858 - mse: 83822328.0000\n",
      "Epoch 150/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 83530830.3393 - mse: 83530816.0000\n",
      "Epoch 151/300\n",
      "2582/2582 [==============================] - 0s 40us/step - loss: 83178724.3594 - mse: 83178712.0000\n",
      "Epoch 152/300\n",
      "2582/2582 [==============================] - 0s 58us/step - loss: 83394667.4888 - mse: 83394664.0000\n",
      "Epoch 153/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 83269708.6785 - mse: 83269704.0000\n",
      "Epoch 154/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 83238518.4818 - mse: 83238528.0000\n",
      "Epoch 155/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 83423307.9721 - mse: 83423296.0000\n",
      "Epoch 156/300\n",
      "2582/2582 [==============================] - 0s 57us/step - loss: 83159147.7119 - mse: 83159152.0000\n",
      "Epoch 157/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 83019050.1131 - mse: 83019048.0000\n",
      "Epoch 158/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 82798754.9001 - mse: 82798752.0000\n",
      "Epoch 159/300\n",
      "2582/2582 [==============================] - 0s 56us/step - loss: 82747193.4934 - mse: 82747200.0000\n",
      "Epoch 160/300\n",
      "2582/2582 [==============================] - 0s 42us/step - loss: 82786024.4419 - mse: 82786032.0000\n",
      "Epoch 161/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 82653058.9558 - mse: 82653064.0000\n",
      "Epoch 162/300\n",
      "2582/2582 [==============================] - 0s 59us/step - loss: 82492395.9179 - mse: 82492384.0000\n",
      "Epoch 163/300\n",
      "2582/2582 [==============================] - 0s 44us/step - loss: 82793266.9032 - mse: 82793264.0000\n",
      "Epoch 164/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 82347020.4307 - mse: 82347024.0000\n",
      "Epoch 165/300\n",
      "2582/2582 [==============================] - 0s 57us/step - loss: 82628163.2068 - mse: 82628168.0000\n",
      "Epoch 166/300\n",
      "2582/2582 [==============================] - 0s 51us/step - loss: 82393451.2037 - mse: 82393440.0000\n",
      "Epoch 167/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 82301505.0658 - mse: 82301496.0000\n",
      "Epoch 168/300\n",
      "2582/2582 [==============================] - 0s 48us/step - loss: 82614676.9698 - mse: 82614680.0000\n",
      "Epoch 169/300\n",
      "2582/2582 [==============================] - 0s 57us/step - loss: 82462373.8125 - mse: 82462376.0000\n",
      "Epoch 170/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 82070652.4462 - mse: 82070656.0000\n",
      "Epoch 171/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 81989979.7242 - mse: 81989984.0000\n",
      "Epoch 172/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 81881903.6685 - mse: 81881904.0000\n",
      "Epoch 173/300\n",
      "2582/2582 [==============================] - 0s 44us/step - loss: 81955564.4988 - mse: 81955560.0000\n",
      "Epoch 174/300\n",
      "2582/2582 [==============================] - 0s 40us/step - loss: 82084965.9148 - mse: 82084960.0000\n",
      "Epoch 175/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 81720515.1077 - mse: 81720520.0000\n",
      "Epoch 176/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 81924151.9070 - mse: 81924152.0000\n",
      "Epoch 177/300\n",
      "2582/2582 [==============================] - 0s 40us/step - loss: 81719120.0991 - mse: 81719120.0000\n",
      "Epoch 178/300\n",
      "2582/2582 [==============================] - 0s 44us/step - loss: 81732932.1146 - mse: 81732920.0000\n",
      "Epoch 179/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 81582079.3431 - mse: 81582088.0000\n",
      "Epoch 180/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 81630816.2045 - mse: 81630832.0000\n",
      "Epoch 181/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 81396118.5438 - mse: 81396104.0000\n",
      "Epoch 182/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 81677402.4291 - mse: 81677400.0000\n",
      "Epoch 183/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 81461983.1201 - mse: 81461968.0000\n",
      "Epoch 184/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 81305441.9628 - mse: 81305448.0000\n",
      "Epoch 185/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 81210083.4144 - mse: 81210080.0000\n",
      "Epoch 186/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 81122178.5438 - mse: 81122184.0000\n",
      "Epoch 187/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 81128833.7413 - mse: 81128832.0000\n",
      "Epoch 188/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 81138219.7583 - mse: 81138216.0000\n",
      "Epoch 189/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 81243081.2781 - mse: 81243096.0000\n",
      "Epoch 190/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 81509985.4748 - mse: 81509976.0000\n",
      "Epoch 191/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 81255402.9558 - mse: 81255392.0000\n",
      "Epoch 192/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 80947755.1046 - mse: 80947760.0000\n",
      "Epoch 193/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 80940473.9272 - mse: 80940472.0000\n",
      "Epoch 194/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 80912256.1053 - mse: 80912264.0000\n",
      "Epoch 195/300\n",
      "2582/2582 [==============================] - 0s 38us/step - loss: 80984341.9148 - mse: 80984344.0000\n",
      "Epoch 196/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 80814931.3710 - mse: 80814952.0000\n",
      "Epoch 197/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 80837651.0178 - mse: 80837648.0000\n",
      "Epoch 198/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 80571911.9256 - mse: 80571912.0000\n",
      "Epoch 199/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 80488125.0132 - mse: 80488128.0000\n",
      "Epoch 200/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 80515076.7529 - mse: 80515072.0000\n",
      "Epoch 201/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 80533147.3710 - mse: 80533144.0000\n",
      "Epoch 202/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 80598946.8598 - mse: 80598952.0000\n",
      "Epoch 203/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 80359437.3664 - mse: 80359432.0000\n",
      "Epoch 204/300\n",
      "2582/2582 [==============================] - 0s 33us/step - loss: 80410952.6321 - mse: 80410952.0000\n",
      "Epoch 205/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 80412230.5190 - mse: 80412216.0000\n",
      "Epoch 206/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 80105602.8621 - mse: 80105592.0000\n",
      "Epoch 207/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 80313678.9713 - mse: 80313680.0000\n",
      "Epoch 208/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 80222782.9527 - mse: 80222768.0000\n",
      "Epoch 209/300\n",
      "2582/2582 [==============================] - 0s 35us/step - loss: 80122612.7653 - mse: 80122608.0000\n",
      "Epoch 210/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 80049622.7049 - mse: 80049616.0000\n",
      "Epoch 211/300\n",
      "2582/2582 [==============================] - 0s 39us/step - loss: 79938737.5399 - mse: 79938736.0000\n",
      "Epoch 212/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 80299357.5507 - mse: 80299352.0000\n",
      "Epoch 213/300\n",
      "2582/2582 [==============================] - 0s 37us/step - loss: 79883581.8528 - mse: 79883592.0000\n",
      "Epoch 214/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 79741989.0937 - mse: 79741992.0000\n",
      "Epoch 215/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 79697567.8761 - mse: 79697568.0000\n",
      "Epoch 216/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 80036325.0891 - mse: 80036320.0000\n",
      "Epoch 217/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 79719347.0798 - mse: 79719352.0000\n",
      "Epoch 218/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 79668094.3036 - mse: 79668096.0000\n",
      "Epoch 219/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 79638578.7947 - mse: 79638584.0000\n",
      "Epoch 220/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 79544958.2649 - mse: 79544960.0000\n",
      "Epoch 221/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 79502411.0426 - mse: 79502408.0000\n",
      "Epoch 222/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 79518520.8056 - mse: 79518520.0000\n",
      "Epoch 223/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 79558084.9620 - mse: 79558080.0000\n",
      "Epoch 224/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 79311030.6367 - mse: 79311032.0000\n",
      "Epoch 225/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 79514111.3153 - mse: 79514104.0000\n",
      "Epoch 226/300\n",
      "2582/2582 [==============================] - 0s 48us/step - loss: 79384238.6584 - mse: 79384248.0000\n",
      "Epoch 227/300\n",
      "2582/2582 [==============================] - 0s 45us/step - loss: 79240259.2285 - mse: 79240264.0000\n",
      "Epoch 228/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 79391798.0480 - mse: 79391792.0000\n",
      "Epoch 229/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 79197262.1658 - mse: 79197256.0000\n",
      "Epoch 230/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 79138258.3826 - mse: 79138264.0000\n",
      "Epoch 231/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 78932655.6034 - mse: 78932648.0000\n",
      "Epoch 232/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 78888631.4113 - mse: 78888624.0000\n",
      "Epoch 233/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 78923109.7630 - mse: 78923104.0000\n",
      "Epoch 234/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 78839689.5926 - mse: 78839704.0000\n",
      "Epoch 235/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 79188571.0240 - mse: 79188552.0000\n",
      "Epoch 236/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 78829189.8931 - mse: 78829200.0000\n",
      "Epoch 237/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 78711420.5174 - mse: 78711424.0000\n",
      "Epoch 238/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 78701691.5120 - mse: 78701696.0000\n",
      "Epoch 239/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 78504474.4384 - mse: 78504472.0000\n",
      "Epoch 240/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 78754147.1665 - mse: 78754144.0000\n",
      "Epoch 241/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 78654356.5298 - mse: 78654352.0000\n",
      "Epoch 242/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 78612551.9938 - mse: 78612544.0000\n",
      "Epoch 243/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 78448363.0581 - mse: 78448360.0000\n",
      "Epoch 244/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 78300649.9024 - mse: 78300656.0000\n",
      "Epoch 245/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 78415143.6220 - mse: 78415152.0000\n",
      "Epoch 246/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 78277751.6282 - mse: 78277752.0000\n",
      "Epoch 247/300\n",
      "2582/2582 [==============================] - 0s 32us/step - loss: 78387426.5221 - mse: 78387432.0000\n",
      "Epoch 248/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 78291449.5678 - mse: 78291448.0000\n",
      "Epoch 249/300\n",
      "2582/2582 [==============================] - 0s 51us/step - loss: 78158533.6886 - mse: 78158528.0000\n",
      "Epoch 250/300\n",
      "2582/2582 [==============================] - 0s 31us/step - loss: 78090294.1224 - mse: 78090296.0000\n",
      "Epoch 251/300\n",
      "2582/2582 [==============================] - 0s 34us/step - loss: 78013460.7033 - mse: 78013456.0000\n",
      "Epoch 252/300\n",
      "2582/2582 [==============================] - 0s 36us/step - loss: 78013479.8265 - mse: 78013480.0000\n",
      "Epoch 253/300\n",
      "2582/2582 [==============================] - 0s 30us/step - loss: 78015125.4950 - mse: 78015128.0000\n",
      "Epoch 254/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 78132849.8095 - mse: 78132848.0000\n",
      "Epoch 255/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 77860546.4725 - mse: 77860552.0000\n",
      "Epoch 256/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 77684631.0054 - mse: 77684640.0000\n",
      "Epoch 257/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 77702400.3346 - mse: 77702416.0000\n",
      "Epoch 258/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 77795164.4245 - mse: 77795152.0000\n",
      "Epoch 259/300\n",
      "2582/2582 [==============================] - 0s 28us/step - loss: 77822415.7955 - mse: 77822432.0000\n",
      "Epoch 260/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 77489086.2711 - mse: 77489080.0000\n",
      "Epoch 261/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 77359201.9675 - mse: 77359208.0000\n",
      "Epoch 262/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 77358908.2789 - mse: 77358912.0000\n",
      "Epoch 263/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 77266960.7188 - mse: 77266960.0000\n",
      "Epoch 264/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 77377374.7916 - mse: 77377376.0000\n",
      "Epoch 265/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 77241575.0395 - mse: 77241584.0000\n",
      "Epoch 266/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 77187887.6716 - mse: 77187888.0000\n",
      "Epoch 267/300\n",
      "2582/2582 [==============================] - 0s 29us/step - loss: 77163957.5337 - mse: 77163944.0000\n",
      "Epoch 268/300\n",
      "2582/2582 [==============================] - 0s 26us/step - loss: 77108402.9001 - mse: 77108400.0000\n",
      "Epoch 269/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 77077522.1131 - mse: 77077528.0000\n",
      "Epoch 270/300\n",
      "2582/2582 [==============================] - 0s 27us/step - loss: 76861880.5763 - mse: 76861880.0000\n",
      "Epoch 271/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 76775080.2912 - mse: 76775072.0000\n",
      "Epoch 272/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 77105561.9396 - mse: 77105568.0000\n",
      "Epoch 273/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 76921092.2231 - mse: 76921096.0000\n",
      "Epoch 274/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 76649903.7149 - mse: 76649904.0000\n",
      "Epoch 275/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76905400.1425 - mse: 76905408.0000\n",
      "Epoch 276/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76797611.5290 - mse: 76797624.0000\n",
      "Epoch 277/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76526616.7870 - mse: 76526608.0000\n",
      "Epoch 278/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 76559457.9644 - mse: 76559472.0000\n",
      "Epoch 279/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 76397343.1263 - mse: 76397352.0000\n",
      "Epoch 280/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76442169.2920 - mse: 76442176.0000\n",
      "Epoch 281/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76309411.6933 - mse: 76309408.0000\n",
      "Epoch 282/300\n",
      "2582/2582 [==============================] - 0s 25us/step - loss: 76135324.4988 - mse: 76135320.0000\n",
      "Epoch 283/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76171334.0728 - mse: 76171336.0000\n",
      "Epoch 284/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2582/2582 [==============================] - 0s 23us/step - loss: 75970559.7041 - mse: 75970552.0000\n",
      "Epoch 285/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 76170923.5198 - mse: 76170920.0000\n",
      "Epoch 286/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 76043378.4291 - mse: 76043368.0000\n",
      "Epoch 287/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75874661.7475 - mse: 75874656.0000\n",
      "Epoch 288/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 76188754.8350 - mse: 76188752.0000\n",
      "Epoch 289/300\n",
      "2582/2582 [==============================] - 0s 22us/step - loss: 75698955.7041 - mse: 75698944.0000\n",
      "Epoch 290/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 75793847.2068 - mse: 75793848.0000\n",
      "Epoch 291/300\n",
      "2582/2582 [==============================] - 0s 22us/step - loss: 75936220.3377 - mse: 75936208.0000\n",
      "Epoch 292/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75782409.1712 - mse: 75782424.0000\n",
      "Epoch 293/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 75561579.9132 - mse: 75561576.0000\n",
      "Epoch 294/300\n",
      "2582/2582 [==============================] - 0s 24us/step - loss: 75552983.5167 - mse: 75552968.0000\n",
      "Epoch 295/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75239973.9737 - mse: 75239992.0000\n",
      "Epoch 296/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75304763.5848 - mse: 75304744.0000\n",
      "Epoch 297/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75294798.4260 - mse: 75294808.0000\n",
      "Epoch 298/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75337838.8722 - mse: 75337824.0000\n",
      "Epoch 299/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75173793.4314 - mse: 75173776.0000\n",
      "Epoch 300/300\n",
      "2582/2582 [==============================] - 0s 23us/step - loss: 75025538.6832 - mse: 75025544.0000\n"
     ]
    }
   ],
   "source": [
    "# olg data:\n",
    "X, y = get_features_target(olg_df)   # get data\n",
    "nn_reg.fit(X, y)     # train model\n",
    "nn_reg.named_steps['estimator'].model.save('olg_nn_reg.h5')  # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_reg_pipeline.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear model from pipeline so pipeline can be saved\n",
    "nn_reg.named_steps['estimator'] = None\n",
    "\n",
    "# save pipeline\n",
    "joblib.dump(nn_reg, 'nn_reg_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
